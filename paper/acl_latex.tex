% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage[review]{latex/acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

\title{Team 21a's Submission to the SIGTYP 2024 Shared Task on Word Embedding Evaluation for Ancient and Historical Languages}

\author{Lester James V. Miranda \\
  Allen Institute for Artificial Intelligence \\
  \texttt{ljm@allenai.org} \\
}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we describe Team 21a's submission to the constrained track of the SIGTYP 2024 Shared Task.
Using only the data provided by the organizers, we built transformer-based multilingual models finetuned on the Universal Dependencies (UD) annotations of a given language.
We also explored the effect of different data mixes, and the cross-lingual capability of our trained models.
% TODO: talk about results and scores on the test set
% maybe benchmark against XLM-RoBERTa and mBERT just for a good baseline?
\end{abstract}

\section{Introduction}
This paper describes Team 21a's submission to the \textit{constrained} track of the SIGTYP 2024 Shared Task on Word Embedding Evaluation for Ancient and Historical Languages.
Our general approach involves pretraining a transformer-based multilingual model on the data mix provided by the organizers, and then finetuning it using the Universal Dependencies (UD) annotations of each language.
We also explored data sampling and data augmentation techniques during the pretraining step to ensure better generalization performance.

Our systems achieved...

We detail our data preprocessing, model pretraining, and finetuning methodologies.
In addition, we also show the results of our cross-lingual transfer learning set-up.

\section{Methodology}


\subsection{Data Preprocessing}


\subsection{Model pretraining}


\subsection{Model finetuning}


\section{Results}


\subsection{Benchmarking results}


\subsection{Cross-lingual transfer}


\subsection{Ablations}


\subsection{}


\bibliography{anthology,custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}
