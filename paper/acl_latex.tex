% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfiles}

% Commands 
\newcommand{\draftonly}[1]{#1}
\newcommand{\draftcomment}[3]{\draftonly{\textcolor{#2}{[#3]{$_{\textsc{#1}}$}}}}
\newcommand{\lj}[1]{\draftcomment{Lj}{violet}{#1}}
\newcommand{\libertus}{\textsc{LiBERTus}}

\title{Team 21a's Submission to the SIGTYP 2024 Shared Task on Word Embedding Evaluation for Ancient and Historical Languages}

\author{Lester James V. Miranda \\
  Allen Institute for Artificial Intelligence \\
  \texttt{ljm@allenai.org} \\
}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we describe Team 21a's submission to the constrained track of the SIGTYP 2024 Shared Task.
Using only the data provided by the organizers, we built transformer-based multilingual models finetuned on the Universal Dependencies (UD) annotations of a given language.
We also explored the cross-lingual capability of our trained models.
\lj{Our systems achieved}
% TODO: talk about results and scores on the test set
% maybe benchmark against XLM-RoBERTa and mBERT just for a good baseline?
\end{abstract}

\section{Introduction}
This paper describes Team 21a's submission to the \textit{constrained} track of the SIGTYP 2024 Shared Task on Word Embedding Evaluation for Ancient and Historical Languages.
Our general approach involves pretraining a transformer-based multilingual model on the shared task dataset, and then finetuning the pretrained model using the Universal Dependencies (UD) annotations of each language.
Throughout this paper, we will refer to the pretrained model as \libertus{}.
We also explored data sampling and augmentation techniques during the pretraining step to ensure better generalization performance.

Our systems achieved...\lj{stuff}


We detail our resource creation, model pretraining, and finetuning methodologies.
In addition, we also show the results of our cross-lingual transfer learning set-up.

\section{Methodology}

% \subfile{tables/full_vs_langspecific.tex}
% \subfile{tables/main_results.tex}


\subsection{Resource creation}

We constructed the pretraining corpora using the annotated tokens of the shared task dataset.
Then, we explored several data augmentation techniques to ensure that each language is properly represented based on the number of unique tokens.

From our experiments, \textbf{upsampling underrepresented languages} helped reduce our pretraining validation loss.
Figure \ref{fig:unique_tokens} shows that \textsc{latm} has the most number of unique tokens in the corpora.
We upsampled each language by randomly duplicating a document in the training pool until the number of unique tokens is greater than or equal to that of \textsc{latm}.
The same figure also shows the token counts after the augmentation process.

\subsection{Model Pretraining}

Using the pretraining corpora, we trained two variants of \libertus{}, a Base model with XXXM parameters and a Large model with XXXM parameters, that serve as foundations for finetuning downstream tasks.
\libertus{} follows RoBERTa's pretraining architecture \cite{liu-etal-2019-roberta} and takes inspiration from \citet{conneau-etal-2020-unsupervised}'s work on scaling BERT models to multiple languages.

Our hyperparameter choices closely resemble that of the original RoBERTa implementation as seen in Table \ref{table:pretrain_hyperparams}.
We also trained the same BPE tokenizer \citep{sennrich-etal-2016-neural} using the constructed corpora.
During model pretraining, we used the AdamW optimizer with $\beta_2$=0.98 and a weight decay of 0.01.
The Base model underwent training for 100,000 steps with a learning rate of 2e-4 whereas the Large variant trained for 300,000 steps.
We used a learning rate scheduler that linearly warms up during the first quarter of the training process, then linearly decays for the rest.
Figure \ref{fig:training_curve} shows the training curve for both variants.

\subfile{tables/pretrain_hyperparams.tex}



\subsection{Model Finetuning}


\section{Results}


\subsection{Benchmarking results}


\subsection{Cross-lingual transfer}


\subsection{Ablations}


\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

\end{document}
