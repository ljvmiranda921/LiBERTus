title: "LiBERTus - A Multilingual Language Model for Ancient and Historical Languages"
description: |
  Submission to Task 1 (Constrained) of the [SIGTYP 2024 Shared Task on Word
  Embedding Evaluation for Ancient and Historical
  Languages](https://sigtyp.github.io/st2024.html)

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "corpus"
  - "metrics"
  - "pretraining"
  - "scripts"
  - "tokenizer"
  - "training"

vars:
  version: 0.1.0
  # Project
  remote_gcs_bucket: "ljvmiranda"
  hf_repo: "ljvmiranda921/LiBERTus"
  # Pretraining params
  seed: 42
  batch_size: 65
  max_steps: 100000
  model_size: "base"
  # Finetuning params
  base_model: "ljvmiranda921/LiBERTus-base"
  train_lang: "chu"
  dev_lang: "chu"
  test_lang: "chu"
  gpu_id: 0
  # Training logs
  name: "main"
  wandb_project: "sigtyp2024"

remotes:
  # Create a service account, download a JSON key, and set the credentials path
  # to GOOGLE_APPLICATION_CREDENTIALS env variable
  gcs: "gs://${vars.remote_gcs_bucket}/research/LiBERTus/training_cache/v${vars.version}/"

# Assets that should be downloaded or available in the directory. But the
# 'weasel assets' command still lets you verify that the checksums match.
assets:
  - dest: assets/train/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/train
    description: "CoNLL-U training datasets for Task 0 (morphology/lemma/POS)"
  - dest: assets/dev/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/valid
    description: "CoNLL-U validation datasets for Task 0 (morphology/lemma/POS)"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "weasel run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  pretrain:
    - create-pretraining
    - create-vocab
    - pretrain-model
  finetune:
    - convert-to-spacy
    - finetune-model
    - evaluate-model

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "weasel run [command] [path]". The help message is optional and
# shown when executing "weasel run [optional command] [path] --help".
commands:
  - name: "create-pretraining"
    help: "Create corpus for multilingual LM pretraining"
    script:
      - >-
        python -m scripts.convert_to_pretrain assets/train/ assets/dev/ 
        --output-path corpus/pretraining.txt 
        --seed ${vars.seed}
        --shuffle
        --verbose
    outputs:
      - corpus/pretraining.txt

  - name: "create-vocab"
    help: "Train a tokenizer to create a vocabulary"
    script:
      - python -m scripts.train_tokenizer corpus/pretraining.txt tokenizer/
    deps:
      - corpus/pretraining.txt
    outputs:
      - tokenizer/vocab.json
      - tokenizer/merges.txt

  - name: "pretrain-model"
    help: "Pretrain a multilingual LM from a corpus"
    script:
      - mkdir -p pretraining/${vars.model_size}/
      - >-
        python -m scripts.pretrain_model pretraining/${vars.model_size}/
        --name ${vars.name}
        --pretraining-corpus corpus/pretraining.txt
        --vocab tokenizer/vocab.json
        --merges tokenizer/merges.txt
        --batch-size ${vars.batch_size}
        --max-steps ${vars.max_steps}
        --seed ${vars.seed}
        --wandb-project ${vars.wandb_project}
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
    outputs:
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin

  - name: "upload-to-hf"
    help: "Upload pretrained model and corresponding tokenizer to the HuggingFace repository"
    script:
      # Upload the models and tokenizer
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} pretraining/${vars.model_size}/model/ .
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} tokenizer/ .
      # Upload pretraining data (for reproducibility)
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} corpus/pretraining.txt pretraining.txt
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin

  - name: "convert-to-spacy"
    help: "Convert CoNLL-U files into spaCy format for finetuning"
    script:
      # Remove erroneus files first from corpus
      - mv assets/train/ohu_train.conllu .
      - mv assets/train/orv_train.conllu .
      - mv assets/dev/orv_valid.conllu .
      - mkdir -p corpus/train
      - python -m spacy convert assets/train/ corpus/train/ --morphology
      - mkdir -p corpus/dev
      - python -m spacy convert assets/dev/ corpus/dev/ --morphology
    deps:
      - assets/train/
      - assets/dev/
    outputs:
      - corpus/train/
      - corpus/dev/

  - name: "finetune-model"
    help: "Finetune a model given a training and validation corpora"
    script:
      - mkdir -p training/${vars.train_lang}/
      - >-
        python -m spacy train configs/default.cfg
        --output-path training/${vars.train_lang}/
        --paths.train corpus/train/${vars.train_lang}_train.spacy
        --paths.dev corpus/dev/${vars.dev_lang}_valid.spacy
        --components.transformer.model.name ${vars.base_model}
        --system.seed ${vars.seed}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/train/${vars.train_lang}_train.spacy
      - corpus/dev/${vars.dev_lang}_valid.spacy
    outputs:
      - training/${vars.train_lang}/model-best/

  - name: "evaluate-model"
    help: "Evaluate a model on both validation and test corpora"
    script:
      - mkdir -p metrics/${vars.train_lang}/
      - >-
        python -m spacy evaluate
        training/${vars.train_lang}/model-best/ corpus/dev/${vars.dev_lang}_valid.spacy
        --output metrics/${vars.train_lang}/metrics-${vars.dev_lang}-dev.json
        --per-component
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy evaluate
        training/${vars.train_lang}/model-best/ corpus/test/${vars.test_lang}_test.spacy
        --output metrics/${vars.train_lang}/metrics-${vars.test_lang}-test.json
        --per-component
        --gpu-id ${vars.gpu_id}
    deps:
      - training/${vars.train_lang}/model-best/
      - corpus/dev/${vars.dev_lang}_valid.spacy
      - corpus/test/${vars.test_lang}_test.spacy
    outputs:
      - metrics/${vars.train_lang}/metrics-${vars.dev_lang}-dev.json
      - metrics/${vars.train_lang}/metrics-${vars.test_lang}-dev.json

  - name: "plot-figures"
    help: "Plot figures for the writeup"
    script:
      - python -m paper.plotters.plot_training_curve metrics/wandb_train_curve_main.csv paper/figures/train_loss.pdf
      - python -m paper.plotters.plot_token_counts metrics/tokens_upsampling.csv paper/figures/token_counts.pdf
    outputs:
      - paper/figures/train_loss.pdf
      - paper/figures/token_counts.pdf
