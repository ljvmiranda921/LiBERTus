title: "LiBERTus - A Multilingual Language Model for Ancient and Historical Languages"
description: |
  Submission to Task 1 (Constrained) of the [SIGTYP 2024 Shared Task on Word
  Embedding Evaluation for Ancient and Historical
  Languages](https://sigtyp.github.io/st2024.html)

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "corpus"
  - "metrics"
  - "pretraining"
  - "scripts"
  - "tokenizer"
  - "training"

vars:
  version: 0.1.0
  # Project
  remote_gcs_bucket: "ljvmiranda"
  hf_repo: "ljvmiranda921/LiBERTus"
  # Training params
  seed: 42
  batch_size: 65
  max_steps: 100000
  model_size: "base"
  # Training logs
  name: "main"
  wandb_project: "sigtyp2024"

remotes:
  # Create a service account, download a JSON key, and set the credentials path
  # to GOOGLE_APPLICATION_CREDENTIALS env variable
  gcs: "gs://${vars.remote_gcs_bucket}/research/LiBERTus/training_cache/v${vars.version}/"

# Assets that should be downloaded or available in the directory. But the
# 'weasel assets' command still lets you verify that the checksums match.
assets:
  - dest: assets/train/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/train
    description: "CoNLL-U training datasets for Task 0 (morphology/lemma/POS)"
  - dest: assets/dev/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/valid
    description: "CoNLL-U validation datasets for Task 0 (morphology/lemma/POS)"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "weasel run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  pretrain:
    - create-pretraining
    - create-vocab
    - pretrain-model

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "weasel run [command] [path]". The help message is optional and
# shown when executing "weasel run [optional command] [path] --help".
commands:
  - name: "create-pretraining"
    help: "Create corpus for multilingual LM pretraining"
    script:
      - >-
        python -m scripts.convert_to_pretrain assets/train/ assets/dev/ 
        --output-path corpus/pretraining.txt 
        --seed ${vars.seed}
        --shuffle
        --verbose
    outputs:
      - corpus/pretraining.txt

  - name: "create-vocab"
    help: "Train a tokenizer to create a vocabulary"
    script:
      - python -m scripts.train_tokenizer corpus/pretraining.txt tokenizer/
    deps:
      - corpus/pretraining.txt
    outputs:
      - tokenizer/vocab.json
      - tokenizer/merges.txt

  - name: "pretrain-model"
    help: "Pretrain a multilingual LM from a corpus"
    script:
      - mkdir -p pretraining/${vars.model_size}/
      - >-
        python -m scripts.pretrain_model pretraining/${vars.model_size}/
        --name ${vars.name}
        --pretraining-corpus corpus/pretraining.txt
        --vocab tokenizer/vocab.json
        --merges tokenizer/merges.txt
        --batch-size ${vars.batch_size}
        --max-steps ${vars.max_steps}
        --seed ${vars.seed}
        --wandb-project ${vars.wandb_project}
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
    outputs:
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin

  - name: "upload-to-hf"
    help: "Upload pretrained model and corresponding tokenizer to the HuggingFace repository"
    script:
      # Upload the models and tokenizer
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} pretraining/${vars.model_size}/model/ .
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} tokenizer/ .
      # Upload pretraining data (for reproducibility)
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} corpus/pretraining.txt pretraining.txt
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin
