title: "LiBERTus - A Multilingual Language Model for Ancient and Historical Languages"
description: |
  Submission to Task 1 (Constrained) of the [SIGTYP 2024 Shared Task on Word
  Embedding Evaluation for Ancient and Historical
  Languages](https://sigtyp.github.io/st2024.html)

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "corpus"
  - "metrics"
  - "pretraining"
  - "scripts"
  - "tokenizer"
  - "training"

vars:
  version: 0.1.0
  # Project
  remote_gcs_bucket: "ljvmiranda"
  # Training params
  seed: 42
  batch_size: 65
  epochs: 5
  # Training logs
  name: "main"
  wandb_project: "sigtyp2024"

remotes:
  # Create a service account, download a JSON key, and set the credentials path
  # to GOOGLE_APPLICATION_CREDENTIALS env variable
  gcs: "gs://${vars.remote_gcs_bucket}/LiBERTus/training_cache/v${vars.version}/"

# Assets that should be downloaded or available in the directory. But the
# 'weasel assets' command still lets you verify that the checksums match.
assets:
  - dest: assets/train/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/train
    description: "CoNLL-U training datasets for Task 0 (morphology/lemma/POS)"
  - dest: assets/dev/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/valid
    description: "CoNLL-U validation datasets for Task 0 (morphology/lemma/POS)"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "weasel run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  pretrain:
    - create-pretraining
    - create-vocab
    - pretrain-model

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "weasel run [command] [path]". The help message is optional and
# shown when executing "weasel run [optional command] [path] --help".
commands:
  - name: "create-pretraining"
    help: "Create corpus for multilingual LM pretraining"
    script:
      - >-
        python -m scripts.create_pretraining assets/train/ assets/dev/ 
        --output-path corpus/pretraining.txt 
        --seed ${vars.seed}
        --shuffle
        --verbose
    outputs:
      - corpus/pretraining.txt

  - name: "create-vocab"
    help: "Train a tokenizer to create a vocabulary"
    script:
      - python -m scripts.create_vocab corpus/pretraining.txt tokenizer/
    deps:
      - corpus/pretraining.txt
    outputs:
      - tokenizer/vocab.json
      - tokenizer/merges.txt

  - name: "pretrain-model"
    help: "Pretrain a multilingual LM from a corpus"
    script:
      - >-
        python -m scripts.pretrain_model pretraining/ 
        --name ${vars.name}
        --pretraining-corpus corpus/pretraining.txt
        --vocab tokenizer/vocab.json
        --merges tokenizer/merges.txt
        --batch-size ${vars.batch_size}
        --epochs ${vars.epochs}
        --seed ${vars.seed}
        --wandb-project ${vars.wandb_project}
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
