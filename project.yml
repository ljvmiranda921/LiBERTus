title: "LiBERTus - A Multilingual Language Model for Ancient and Historical Languages"
description: |
  Submission to Task 1 (Constrained) of the [SIGTYP 2024 Shared Task on Word
  Embedding Evaluation for Ancient and Historical
  Languages](https://sigtyp.github.io/st2024.html)

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  - "assets"
  - "corpus"
  - "metrics"
  - "packages"
  - "pretraining"
  - "scripts"
  - "tokenizer"
  - "training"

vars:
  version: 0.1.0
  # Project
  remote_gcs_bucket: "ljvmiranda"
  hf_repo: "ljvmiranda921/LiBERTus"
  publish_message: "Update SIGTYP pipeline"
  # Pretraining params
  seed: 42
  batch_size: 65
  max_steps: 100000
  save_steps: 1000
  model_size: "base"
  sampling_strategy: "upsample"
  # Finetuning params
  config: "transformer.cfg"
  base_model: "ljvmiranda921/LiBERTus-base"
  train_lang: "chu"
  dev_lang: "chu"
  test_lang: "chu"
  gpu_id: 0
  # Training logs
  name: "main"
  wandb_project: "sigtyp2024"

env:
  HUGGINGFACE_TOKEN: HUGGINGFACE_TOKEN
  WANDB_LOG_MODEL: WANDB_LOG_MODEL

remotes:
  # Create a service account, download a JSON key, and set the credentials path
  # to GOOGLE_APPLICATION_CREDENTIALS env variable
  gcs: "gs://${vars.remote_gcs_bucket}/research/LiBERTus/training_cache/v${vars.version}/"

# Assets that should be downloaded or available in the directory. But the
# 'weasel assets' command still lets you verify that the checksums match.
assets:
  - dest: assets/train/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/train
    description: "CoNLL-U training datasets for Task 0 (morphology/lemma/POS)"
  - dest: assets/dev/
    git:
      repo: https://github.com/sigtyp/ST2024
      branch: main
      path: morphology/valid
    description: "CoNLL-U validation datasets for Task 0 (morphology/lemma/POS)"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "weasel run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  pretrain:
    - create-pretraining
    - create-vocab
    - pretrain-model
  finetune:
    - convert-to-spacy
    - finetune-trf-model
    - evaluate-model-dev
    # - evaluate-model-test

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "weasel run [command] [path]". The help message is optional and
# shown when executing "weasel run [optional command] [path] --help".
commands:
  - name: "create-pretraining"
    help: "Create corpus for multilingual LM pretraining"
    script:
      - >-
        python -m scripts.convert_to_pretrain assets/train/ assets/dev/ 
        --output-path corpus/pretraining.txt 
        --seed ${vars.seed}
        --sampling-strategy ${vars.sampling_strategy}
        --shuffle
        --verbose
    outputs:
      - corpus/pretraining.txt

  - name: "create-vocab"
    help: "Train a tokenizer to create a vocabulary"
    script:
      - python -m scripts.train_tokenizer corpus/pretraining.txt tokenizer/
    deps:
      - corpus/pretraining.txt
    outputs:
      - tokenizer/vocab.json
      - tokenizer/merges.txt

  - name: "pretrain-model"
    help: "Pretrain a multilingual LM from a corpus"
    script:
      - mkdir -p pretraining/${vars.model_size}/
      - >-
        python -m scripts.pretrain_model pretraining/${vars.model_size}/
        --name ${vars.name}
        --size ${vars.model_size}
        --pretraining-corpus corpus/pretraining.txt
        --vocab tokenizer/vocab.json
        --merges tokenizer/merges.txt
        --batch-size ${vars.batch_size}
        --max-steps ${vars.max_steps}
        --save-steps ${vars.save_steps}
        --seed ${vars.seed}
        --wandb-project ${vars.wandb_project}
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
    outputs:
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin

  - name: "pretrain-model-from-checkpoint"
    help: "Pretrain a multilingual LM from a corpus based on a checkpoint"
    script:
      - mkdir -p pretraining/${vars.model_size}/
      - >-
        python -m scripts.pretrain_model pretraining/${vars.model_size}/
        --resume-from-checkpoint
        --name ${vars.name}
        --size ${vars.model_size}
        --pretraining-corpus corpus/pretraining.txt
        --vocab tokenizer/vocab.json
        --merges tokenizer/merges.txt
        --batch-size ${vars.batch_size}
        --max-steps ${vars.max_steps}
        --save-steps ${vars.save_steps}
        --seed ${vars.seed}
        --wandb-project ${vars.wandb_project}
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
    outputs:
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin

  - name: "upload-to-hf"
    help: "Upload pretrained model and corresponding tokenizer to the HuggingFace repository"
    script:
      # Upload the models and tokenizer
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} pretraining/${vars.model_size}/model/ .
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} tokenizer/ .
      # Upload pretraining data (for reproducibility)
      - huggingface-cli upload ${vars.hf_repo}-${vars.model_size} corpus/pretraining.txt pretraining.txt
    deps:
      - tokenizer/vocab.json
      - tokenizer/merges.txt
      - corpus/pretraining.txt
      - pretraining/${vars.model_size}/model/config.json
      - pretraining/${vars.model_size}/model/model.safetensors
      - pretraining/${vars.model_size}/model/training_args.bin

  - name: "convert-to-spacy"
    help: "Convert CoNLL-U files into spaCy format for finetuning"
    script:
      # Remove erroneus files first from corpus
      - mv assets/train/ohu_train.conllu .
      - mv assets/train/orv_train.conllu .
      - mv assets/dev/orv_valid.conllu .
      - mkdir -p corpus/train
      - python -m spacy convert assets/train/ corpus/train/ --morphology
      - mkdir -p corpus/dev
      - python -m spacy convert assets/dev/ corpus/dev/ --morphology
      # Concatenate everything for experiment
      - mkdir -p corpus/merged_train/
      - python -m spacy convert assets/train/ corpus/merged_train/ --morphology --concatenate
      - mkdir -p corpus/merged_dev/
      - python -m spacy convert assets/dev/ corpus/merged_dev/ --morphology --concatenate
    deps:
      - assets/train/
      - assets/dev/
    outputs:
      - corpus/train/
      - corpus/dev/
      - corpus/merged_train/
      - corpus/merged_dev/

  - name: "finetune-tok2vec-model"
    help: "Finetune a tok2vec model given a training and validation corpora"
    script:
      - mkdir -p training/${vars.train_lang}/
      - >-
        python -m spacy train configs/default.cfg
        --output-path training/${vars.train_lang}/
        --paths.train corpus/train/${vars.train_lang}_train.spacy
        --paths.dev corpus/dev/${vars.dev_lang}_valid.spacy
        --system.seed ${vars.seed}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/train/${vars.train_lang}_train.spacy
      - corpus/dev/${vars.dev_lang}_valid.spacy
    outputs:
      - training/${vars.train_lang}/model-best/
      - training/${vars.train_lang}/model-last/

  - name: "finetune-trf-model"
    help: "Finetune a transformer model given a training and validation corpora"
    script:
      - mkdir -p training/${vars.train_lang}/
      - >-
        python -m spacy train configs/transformer.cfg
        --output-path training/${vars.train_lang}/
        --paths.train corpus/train/${vars.train_lang}_train.spacy
        --paths.dev corpus/dev/${vars.dev_lang}_valid.spacy
        --components.transformer.model.name ${vars.base_model}
        --system.seed ${vars.seed}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/train/${vars.train_lang}_train.spacy
      - corpus/dev/${vars.dev_lang}_valid.spacy
    outputs:
      - training/${vars.train_lang}/model-best/
      - training/${vars.train_lang}/model-last/

  - name: "package-model"
    help: "Package model and upload to HuggingFace"
    script:
      - >-
        python -m spacy package
        training/${vars.train_lang}/model-best/ packages/
        --name ${vars.train_lang}_sigtyp_trf
        --version ${vars.version}
        --build sdist,wheel
        --force
      # Upload to HuggingFace
      - huggingface-cli login --token ${env.HUGGINGFACE_TOKEN}
      - >-
        python -m spacy huggingface-hub push packages/xx_${vars.train_lang}_sigtyp_trf-${vars.version}/dist/xx_${vars.train_lang}_sigtyp_trf-${vars.version}-py3-none-any.whl
        --msg "xx_${vars.train_lang}_sigtyp_trf-${vars.version}: ${vars.publish_message}"
        --verbose
    deps:
      - training/${vars.train_lang}/model-best/
    outputs:
      - packages/xx_${vars.train_lang}_sigtyp_trf-${vars.version}/dist/${vars.train_lang}_sigtyp_trf-${vars.version}.tar.gz
      - packages/xx_${vars.train_lang}_sigtyp_trf-${vars.version}/dist/${vars.train_lang}_sigtyp_trf-${vars.version}-py3-none-any.whl

  - name: "evaluate-model-dev"
    help: "Evaluate a model on the validation set"
    script:
      - mkdir -p metrics/system_scores/${vars.train_lang}/
      - >-
        python -m spacy evaluate
        training/${vars.train_lang}/model-best/ corpus/dev/${vars.dev_lang}_valid.spacy
        --output metrics/system_scores/${vars.train_lang}/metrics-${vars.dev_lang}-dev.json
        --per-component
        --gpu-id ${vars.gpu_id}
    deps:
      - training/${vars.train_lang}/model-best/
      - corpus/dev/${vars.dev_lang}_valid.spacy
    outputs:
      - metrics/system_scores/${vars.train_lang}/metrics-${vars.dev_lang}-dev.json

  - name: "evaluate-model-test"
    help: "Evaluate a model on the test set"
    script:
      - mkdir -p metrics/system_scores/${vars.train_lang}/
      - >-
        python -m spacy evaluate
        training/${vars.train_lang}/model-best/ corpus/test/${vars.test_lang}_test.spacy
        --output metrics/system_scores/${vars.train_lang}/metrics-${vars.test_lang}-test.json
        --per-component
        --gpu-id ${vars.gpu_id}
    deps:
      - training/${vars.train_lang}/model-best/
      - corpus/test/${vars.test_lang}_test.spacy
    outputs:
      - metrics/system_scores/${vars.train_lang}/metrics-${vars.test_lang}-dev.json

  - name: "plot-figures"
    help: "Plot figures for the writeup"
    script:
      - python -m scripts.plotters.plot_training_curve metrics/wandb_train_curve_main.csv paper/figures/train_loss.pdf
      - python -m scripts.plotters.plot_token_counts metrics/tokens_upsampling.csv paper/figures/token_counts.pdf
      - python -m scripts.plotters.plot_crosseval_heatmap metrics/system_scores/ paper/figures/cross_lingual.pdf
      - python -m scripts.plotters.plot_hashembed_comparison metrics/system_scores/ metrics/hashembed_scores/ paper/figures/hashembed.pdf
    outputs:
      - paper/figures/train_loss.pdf
      - paper/figures/token_counts.pdf
      - paper/figures/cross_lingual.pdf
      - paper/figures/hashembed.pdf
